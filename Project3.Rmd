---
title: "Data 607 - Project 3"
author: "Cameron Smith"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r load-packages, message=FALSE}
library(tidyverse)
library(RPostgreSQL)
library(DBI)
library(tm)
library(SnowballC)
library(wordcloud)
library(RCurl)
library(RColorBrewer)
```

# Introduction and Approach

This project is a group project with the following members:

* Cameron Smith
* Brett D
* Darius Z

Formal citation for data source used:

JobsPikr. (2019; December).  10000 Data Scientist Job Postings from the USA, Version 1.  Retrieved 15 October 2020 from https://www.kaggle.com/jobspikr/data-scientist-job-postings-from-the-usa.

## Load the data

A Google Cloud Postgres database was created for this project to maximize reproducibility.  The below code loads the data from the normalized database and converts it into data frames that can be used for further analysis.

This required (1) setting up a Postgres database; (2) creating a storage bucket; (3) uploading the raw data to the storage bucket; (4) migrating that raw data into the new database, which consists of two tables in the first normal form 1NF; and (5) opening up the firewall to allow public access.  Per the rules for 1NF, each table cell should contain a single value and each record needs to be unique.  Both of these conditions are satisfied and thus the data can be considered normalized.

### Get the data from the database

```{r}
# Configure database connection
db <- "postgres"
host_db <- "35.243.187.235"
db_port <- "5432"

# Using public account created with read only access
db_user <- "project3public"
db_password <- "cunymsds2020"

con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)

# Verify the connection to the database is working
dbListTables(con)

# Get job posting data
jobdata_raw <- 
  dbGetQuery(con, "SELECT job_title, job_description, category, company, city, state, job_board, post_date FROM jobs")

# Get skills data
skills_df <- dbGetQuery(con, "SELECT skill_name FROM skills")

# Disconnect from the database
dbDisconnect(con) 

```

## Tidy and Transform

With the data now loaded into a data frame, it needs to be prepared for analysis.

Main rules of 'tidyness': 

* Each variable must have its own column
* Each observation must have its own row
* Each value must have its own cell

Following these rules we end up with a very wide data set in this instance.

```{r}
# Start with a bit of exploratory data analysis
glimpse(jobdata_raw)

# Summary of job postings by site
jobdata_raw %>% 
  select(job_board) %>% 
  group_by(job_board) %>% 
  summarise(num_postings = n()) %>%
  ggplot(aes(x = job_board, y = num_postings)) +
    geom_bar(stat = 'identity')
```

Prepare the data frame to capture the data needed.

```{r}
# Copy the raw data into a new data frame to preserve the original
jobdata_tidy <- jobdata_raw

# Create wide data frame with new columns and blank based on each skill name
for (i in 1:nrow(skills_df)){
  tempvar = skills_df$skill_name[i]
  jobdata_tidy[[tempvar]] <- ""
}

```

Now that the data frame is in the intended format, with the key columns created, we will search through each job listing to identify whether each skill is listed, then add it to a count in the columns created.

We will loop through the 40 something skills of interest, and for each one, we will set its column to either true or false depending on if it shows up in the job description.  **this operation can take up to 1 minute to complete**

```{r}
for(s in 1:nrow(skills_df)) {
  skill = skills_df[s, 1]
  jobdata_tidy[skill] = grepl(tolower(paste("", skill, "", sep=" ")), tolower(jobdata_tidy$job_description), fixed=TRUE)
}

```


With the data calculated and populated, a new data frame will be created in a longer format for easier analysis.

```{r message=FALSE, warning=FALSE}
# Create long data frame with key data for easier analysis
skillsdata_long <- jobdata_tidy %>%
  select(9:54) %>%
  gather ("skill_name", "exists", 1:46) %>%
  filter(exists == TRUE)

```

## Analysis

AS you can see, the most sought after skills for a data scientist according to our data is Python and SQL, with everything else at least 50 percent behind.  I think that it's interesting that Python has become so pivotal in this field, surpassing even R.

```{r message=FALSE}
skillssummary <- skillsdata_long %>% 
  group_by(skill_name) %>%
  summarise(num_postings = n())

# Visualize in descending order the popularity of all skills identified

skillssummary %>%  
  ggplot(aes(x = reorder(skill_name, num_postings), y = num_postings)) +
  geom_bar(stat = 'identity') +
  coord_flip()
```


## Conclusion

In conclusion, based on a consolidated list of 10,000 data science job postings from a variety of websites, the top 10 data science skills are:

```{r}
top_n(arrange(skillssummary, desc(num_postings)), 10)
```

Make a word cloud from some of the job descriptions
```{r}
write(jobdata_tidy[1:1250,]$job_description, "info.txt")
filePath <- "info.txt"
res<-rquery.wordcloud(filePath, type ="file", lang = "english", max.words = 50)
```

Something interesting here is the popping up of the word "machine" and "learning" which is not a skill, but clearly machine learning is another important role that data scientists are expected to fill.


Python, SQL and R are the most important languages to learn, cross referencing this data against the tiobe index of programming languages (https://www.tiobe.com/tiobe-index/) shows that python is number 3 overall as well, making this not just a useful language to know for data science, but other disciplines of computer science as well.
